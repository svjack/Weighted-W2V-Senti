{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_path = \"w2v_douban.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec.load(w2v_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"DMSC.csv\", index_col = 0)\n",
    "sample_df = data.groupby(\n",
    "    [\"Movie_Name_CN\", \"Star\"]\n",
    ").apply(\n",
    "    lambda x: x.sample(n = int(2125056/(28*200)), replace = True, random_state = 0)\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "comments = sample_df.values[:, 7]\n",
    "star = sample_df.values[:, 6]\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    comments, star, test_size = 0.2, random_state = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-08-08 02:46:40--  https://codeload.github.com/weiyunchen/nlp/zip/master\n",
      "Resolving codeload.github.com (codeload.github.com)... 140.82.112.9\n",
      "Connecting to codeload.github.com (codeload.github.com)|140.82.112.9|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/zip]\n",
      "Saving to: ‘master’\n",
      "\n",
      "master                  [ <=>                ]  75.41K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2020-08-08 02:46:40 (2.92 MB/s) - ‘master’ saved [77224]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -nc \"https://codeload.github.com/weiyunchen/nlp/zip/master\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  master\n",
      "78a55b3ff658cd83cd1ae4abbe5fb475507097c3\n",
      "   creating: nlp-master/\n",
      "  inflating: nlp-master/faker.png    \n",
      "  inflating: nlp-master/stopwords.txt  \n"
     ]
    }
   ],
   "source": [
    "!unzip master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /content/gdrive/'My Drive'/douban* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['douban_data.pkl', 'douban_comment_cut.txt', 'douban_word_senti_dict.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "glob(\"douban*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read num 100000 set size 71870\n",
      "read num 200000 set size 106359\n",
      "read num 300000 set size 129472\n",
      "read num 400000 set size 146686\n",
      "read num 500000 set size 163406\n",
      "read num 600000 set size 181303\n",
      "read num 700000 set size 197381\n",
      "read num 800000 set size 210911\n",
      "read num 900000 set size 226422\n",
      "read num 1000000 set size 239623\n",
      "read num 1100000 set size 251503\n",
      "read num 1200000 set size 262019\n",
      "read num 1300000 set size 274062\n",
      "read num 1400000 set size 284474\n",
      "read num 1500000 set size 295874\n",
      "read num 1600000 set size 306323\n",
      "read num 1700000 set size 316634\n",
      "read num 1800000 set size 325081\n"
     ]
    }
   ],
   "source": [
    "comment_cut_path = \"douban_comment_cut.txt\"\n",
    "comment_distinct_words = set([])\n",
    "with open(comment_cut_path, \"r\") as f:\n",
    "    idx = 0\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if line:\n",
    "            idx += 1\n",
    "            if idx > 0 and idx % 100000 == 0:\n",
    "                print(\"read num {} set size {}\".format(idx, len(comment_distinct_words)))\n",
    "            for w in line.split(\" \"):\n",
    "                comment_distinct_words.add(w)\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write num 100000\n",
      "write num 200000\n",
      "write num 300000\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "wiki_dict_path = 'wiki-dict.txt'\n",
    "if os.path.exists(wiki_dict_path):\n",
    "    os.remove(wiki_dict_path)\n",
    "    \n",
    "with open(wiki_dict_path, 'w') as f:\n",
    "    for idx, w in enumerate(comment_distinct_words):\n",
    "        if hasattr(w, \"text\"):\n",
    "            #### skip entity\n",
    "            f.write(u\"{}\\n\".format(w.text))\n",
    "        if idx > 0 and idx % 100000 == 0:\n",
    "            print(\"write num {}\".format(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.877 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "jieba.load_userdict(wiki_dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab6406ec136e4e57ae1a7cd8187995f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=42448.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf64422a15484b119c728feebb8d891a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10612.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"nlp-master/stopwords.txt\", \"r\") as f:\n",
    "    stopwords = list(map(lambda line: line.strip(\"\\n\") ,f.readlines()))\n",
    "    \n",
    "def cut(data, labels, stopwords):\n",
    "    result = []\n",
    "    new_labels = []\n",
    "    for index in tqdm_notebook(range(len(data))):\n",
    "        comment = clean_str(data[index])\n",
    "        label = labels[index]\n",
    "        seg_list = jieba.cut(comment, cut_all = False, HMM = True)\n",
    "        seg_list = list(filter(lambda x: x.strip(\"\\n\") if (x not in stopwords and len(x) > 1) else None, seg_list))\n",
    "        if len(seg_list) > 1:\n",
    "            result.append(seg_list)\n",
    "            new_labels.append(label)\n",
    "    return result, new_labels\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import re\n",
    "\n",
    "def clean_str(line):\n",
    "    line.strip('\\n')\n",
    "    line = re.sub(r\"[^\\u4e00-\\u9fff]\", \"\", line)\n",
    "    line = re.sub(\n",
    "        \"[0-9a-zA-Z\\-\\s+\\.\\!\\/_,$%^*\\(\\)\\+(+\\\"\\')]+|[+——！，。？、~@#￥%……&*（）<>\\[\\]:：★◆【】《》;；=?？]+\", \"\", line)\n",
    "    return line.strip()\n",
    "\n",
    "train_cut_result, train_labels = cut(x_train, y_train, stopwords)\n",
    "test_cut_result, test_labels = cut(x_test, y_test, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traditional_clf_score(name, train_vec_data, train_labels, test_vec_data, test_labels, use_two_class = True):\n",
    "    train_labels, train_labels = map(np.asarray, [train_labels, train_labels])\n",
    "    if use_two_class:\n",
    "        train_labels, test_labels = map(lambda x: pd.Series(x).map(lambda xx: int(xx <= 2)), [train_labels, test_labels])\n",
    "    clf = None\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "    assert name in [\"SVC\", \"decision_tree\", \"NB\", \"random_forest\", \"MLP\"]\n",
    "    if name == \"SVC\":\n",
    "        #clf = SVC(kernel = \"linear\")\n",
    "        clf = SVC(kernel = \"rbf\")\n",
    "    elif name == \"decision_tree\":\n",
    "        clf = DecisionTreeClassifier()\n",
    "    elif name == \"NB\":\n",
    "        clf = GaussianNB()\n",
    "    elif name == \"random_forest\":\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators = 100, \n",
    "        )\n",
    "    elif name == \"MLP\":\n",
    "        clf = MLPClassifier(hidden_layer_sizes=(300, 300, 300, 150), activation = \"identity\")\n",
    "    clf.fit(train_vec_data, train_labels)\n",
    "    pred = clf.predict(test_vec_data)\n",
    "    acc_score = accuracy_score(test_labels, pred)\n",
    "    balance_acc_score = balanced_accuracy_score(test_labels, pred)\n",
    "    return {\"acc\": acc_score, \"b_acc\": balance_acc_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.6407734436123841, 'b_acc': 0.6260695057252395}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mean_list = list(map(lambda x: np.concatenate(list(map(lambda y: w2v_model.wv[y].reshape([1, -1]) if y in w2v_model.wv else np.zeros((1,300)), x) ), axis = 0).mean(axis = 0),train_cut_result))\n",
    "test_mean_list = list(map(lambda x: np.concatenate(list(map(lambda y: w2v_model.wv[y].reshape([1, -1]) if y in w2v_model.wv else np.zeros((1,300)), x) ), axis = 0).mean(axis = 0),test_cut_result))\n",
    "\n",
    "train_X, test_X = np.stack(train_mean_list), np.stack(test_mean_list)\n",
    "train_y, test_y = map(np.asarray, [train_labels, test_labels])\n",
    "traditional_clf_score(\"decision_tree\", train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_df = pd.read_csv(\"douban_word_senti_dict.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_score_dict = dict((r[\"index\"], r[\"score_mean\"]) for _, r in req_df[[\"score_mean\"]].reset_index().iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6428359553083972"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "req_score_dict[u\"电影\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.6618978428523528, 'b_acc': 0.6494268923767731}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_array = np.stack(list(map(lambda x: np.concatenate(list(map(lambda y: w2v_model.wv[y].reshape([1, -1]) if y in w2v_model.wv else np.zeros((1,300)), x) ), axis = 0).mean(axis = 0),train_cut_result + test_cut_result))).mean(axis = 0)\n",
    "\n",
    "empty_array = np.abs(empty_array).reshape((1, 300))\n",
    "#empty_array = np.zeros((1, 300))\n",
    "empty_weight = np.percentile(list(req_score_dict.values()), 50)\n",
    "#empty_weight = 0.0\n",
    "\n",
    "train_mean_list = list(map(lambda x: np.concatenate(list(map(lambda y: np.abs(w2v_model.wv[y].reshape([1, -1])) * req_score_dict.get(y, empty_weight) if y in w2v_model.wv else empty_array, x) ), axis = 0).mean(axis = 0),train_cut_result))\n",
    "\n",
    "test_mean_list = list(map(lambda x: np.concatenate(list(map(lambda y: np.abs(w2v_model.wv[y].reshape([1, -1])) * req_score_dict.get(y, empty_weight) if y in w2v_model.wv else empty_array, x) ), axis = 0).mean(axis = 0),test_cut_result))\n",
    "\n",
    "train_X, test_X = np.stack(train_mean_list), np.stack(test_mean_list)\n",
    "\n",
    "train_y, test_y = map(np.asarray, [train_labels, test_labels])\n",
    "traditional_clf_score(\"decision_tree\", train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.6810103945456578, 'b_acc': 0.6665758716479548}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_value = np.stack(list(map(lambda x: np.concatenate(list(map(lambda y: w2v_model.wv[y].reshape([1, -1]) if y in w2v_model.wv else np.zeros((1,300)), x) ), axis = 0).min(axis = 0),train_cut_result + test_cut_result))).min()\n",
    "\n",
    "abs_min_value = np.abs(min_value)\n",
    "\n",
    "empty_array = np.stack(list(map(lambda x: np.concatenate(list(map(lambda y: w2v_model.wv[y].reshape([1, -1]) if y in w2v_model.wv else np.zeros((1,300)), x) ), axis = 0).mean(axis = 0),train_cut_result + test_cut_result))).mean(axis = 0) + abs_min_value\n",
    "\n",
    "assert np.all(empty_array > 0)\n",
    "\n",
    "empty_array = empty_array.reshape((1, 300))\n",
    "#empty_array = np.zeros((1, 300))\n",
    "empty_weight = np.percentile(list(req_score_dict.values()), 50)\n",
    "#empty_weight = 1.0\n",
    "\n",
    "#req_score_dict = dict()\n",
    "\n",
    "train_mean_list = list(map(lambda x: np.concatenate(list(map(lambda y: (w2v_model.wv[y].reshape([1, -1]) + abs_min_value) * req_score_dict.get(y, empty_weight) if y in w2v_model.wv else empty_array, x) ), axis = 0).mean(axis = 0),train_cut_result))\n",
    "\n",
    "test_mean_list = list(map(lambda x: np.concatenate(list(map(lambda y: (w2v_model.wv[y].reshape([1, -1]) + abs_min_value) * req_score_dict.get(y, empty_weight) if y in w2v_model.wv else empty_array, x) ), axis = 0).mean(axis = 0),test_cut_result))\n",
    "\n",
    "train_X, test_X = np.stack(train_mean_list), np.stack(test_mean_list)\n",
    "\n",
    "train_y, test_y = map(np.asarray, [train_labels, test_labels])\n",
    "traditional_clf_score(\"decision_tree\", train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.692634402593048, 'b_acc': 0.6811684277745258}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_value = np.stack(list(map(lambda x: np.concatenate(list(map(lambda y: w2v_model.wv[y].reshape([1, -1]) if y in w2v_model.wv else np.zeros((1,300)), x) ), axis = 0).min(axis = 0),train_cut_result + test_cut_result))).min()\n",
    "\n",
    "abs_min_value = np.abs(min_value)\n",
    "\n",
    "empty_array = np.stack(list(map(lambda x: np.concatenate(list(map(lambda y: w2v_model.wv[y].reshape([1, -1]) if y in w2v_model.wv else np.zeros((1,300)), x) ), axis = 0).mean(axis = 0),train_cut_result + test_cut_result))).mean(axis = 0) + abs_min_value\n",
    "\n",
    "assert np.all(empty_array > 0)\n",
    "\n",
    "empty_array = empty_array.reshape((1, 300))\n",
    "#empty_array = np.zeros((1, 300))\n",
    "empty_weight = np.percentile(list(req_score_dict.values()), 50)\n",
    "#empty_weight = 1.0\n",
    "\n",
    "train_mean_list = list(map(lambda x: np.concatenate(list(map(lambda y: (w2v_model.wv[y].reshape([1, -1]) + abs_min_value) * req_score_dict.get(y, empty_weight) if y in w2v_model.wv else empty_array, x) ), axis = 0).mean(axis = 0),train_cut_result))\n",
    "\n",
    "test_mean_list = list(map(lambda x: np.concatenate(list(map(lambda y: (w2v_model.wv[y].reshape([1, -1]) + abs_min_value) * req_score_dict.get(y, empty_weight) if y in w2v_model.wv else empty_array, x) ), axis = 0).mean(axis = 0),test_cut_result))\n",
    "\n",
    "train_X, test_X = np.stack(train_mean_list), np.stack(test_mean_list)\n",
    "\n",
    "train_mean_list = list(map(lambda x: np.concatenate(list(map(lambda y: w2v_model.wv[y].reshape([1, -1]) if y in w2v_model.wv else np.zeros((1,300)), x) ), axis = 0).mean(axis = 0),train_cut_result))\n",
    "test_mean_list = list(map(lambda x: np.concatenate(list(map(lambda y: w2v_model.wv[y].reshape([1, -1]) if y in w2v_model.wv else np.zeros((1,300)), x) ), axis = 0).mean(axis = 0),test_cut_result))\n",
    "\n",
    "train_X_append, test_X_append = np.stack(train_mean_list), np.stack(test_mean_list)\n",
    "\n",
    "train_X = np.concatenate([train_X, train_X_append], axis = 1)\n",
    "\n",
    "test_X = np.concatenate([test_X, test_X_append], axis = 1)\n",
    "\n",
    "train_score_sum = list(map(lambda inner_list: sum(map(lambda x: req_score_dict[x] if x in req_score_dict else empty_weight,inner_list)) ,train_cut_result))\n",
    "\n",
    "test_score_sum = list(map(lambda inner_list: sum(map(lambda x: req_score_dict[x] if x in req_score_dict else empty_weight,inner_list)) ,test_cut_result))\n",
    "\n",
    "train_score_max = list(map(lambda inner_list: max(map(lambda x: req_score_dict[x] if x in req_score_dict else empty_weight,inner_list)) ,train_cut_result))\n",
    "\n",
    "test_score_max = list(map(lambda inner_list: max(map(lambda x: req_score_dict[x] if x in req_score_dict else empty_weight,inner_list)) ,test_cut_result))\n",
    "\n",
    "train_score_min = list(map(lambda inner_list: min(map(lambda x: req_score_dict[x] if x in req_score_dict else empty_weight,inner_list)) ,train_cut_result))\n",
    "\n",
    "test_score_min = list(map(lambda inner_list: min(map(lambda x: req_score_dict[x] if x in req_score_dict else empty_weight,inner_list)) ,test_cut_result))\n",
    "\n",
    "train_score_mean = list(map(lambda inner_list: np.mean(list(map(lambda x: req_score_dict[x] if x in req_score_dict else empty_weight,inner_list))) ,train_cut_result))\n",
    "\n",
    "test_score_mean = list(map(lambda inner_list: np.mean(list(map(lambda x: req_score_dict[x] if x in req_score_dict else empty_weight,inner_list))) ,test_cut_result))\n",
    "\n",
    "train_X = np.concatenate([train_X, np.asarray(train_score_sum).reshape([-1, 1])], axis = 1)\n",
    "\n",
    "test_X = np.concatenate([test_X, np.asarray(test_score_sum).reshape([-1, 1])], axis = 1)\n",
    "\n",
    "train_X = np.concatenate([train_X, np.asarray(train_score_max).reshape([-1, 1])], axis = 1)\n",
    "\n",
    "test_X = np.concatenate([test_X, np.asarray(test_score_max).reshape([-1, 1])], axis = 1)\n",
    "\n",
    "train_X = np.concatenate([train_X, np.asarray(train_score_min).reshape([-1, 1])], axis = 1)\n",
    "\n",
    "test_X = np.concatenate([test_X, np.asarray(test_score_min).reshape([-1, 1])], axis = 1)\n",
    "\n",
    "train_X = np.concatenate([train_X, np.asarray(train_score_mean).reshape([-1, 1])], axis = 1)\n",
    "\n",
    "test_X = np.concatenate([test_X, np.asarray(test_score_mean).reshape([-1, 1])], axis = 1)\n",
    "\n",
    "train_y, test_y = map(np.asarray, [train_labels, test_labels])\n",
    "traditional_clf_score(\"decision_tree\", train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.7812674639543982, 'b_acc': 0.7575754070158398}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traditional_clf_score(\"random_forest\", train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.7670727618196044, 'b_acc': 0.7511091014679556}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def traditional_clf_score(name, train_vec_data, train_labels, test_vec_data, test_labels, use_two_class = True):\n",
    "    train_labels, train_labels = map(np.asarray, [train_labels, train_labels])\n",
    "    if use_two_class:\n",
    "        train_labels, test_labels = map(lambda x: pd.Series(x).map(lambda xx: int(xx <= 2)), [train_labels, test_labels])\n",
    "    clf = None\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "    assert name in [\"SVC\", \"decision_tree\", \"NB\", \"random_forest\", \"MLP\"]\n",
    "    if name == \"SVC\":\n",
    "        #clf = SVC(kernel = \"linear\")\n",
    "        clf = SVC(kernel = \"rbf\")\n",
    "    elif name == \"decision_tree\":\n",
    "        clf = DecisionTreeClassifier()\n",
    "    elif name == \"NB\":\n",
    "        clf = GaussianNB()\n",
    "    elif name == \"random_forest\":\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators = 100, \n",
    "        )\n",
    "    elif name == \"MLP\":\n",
    "        clf = MLPClassifier(hidden_layer_sizes=(300, 300, 300, 150), activation = \"identity\")\n",
    "    clf.fit(train_vec_data, train_labels)\n",
    "    pred = clf.predict(test_vec_data)\n",
    "    acc_score = accuracy_score(test_labels, pred)\n",
    "    balance_acc_score = balanced_accuracy_score(test_labels, pred)\n",
    "    return {\"acc\": acc_score, \"b_acc\": balance_acc_score}\n",
    "\n",
    "traditional_clf_score(\"MLP\", train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traditional_clf_score(name, train_vec_data, train_labels, test_vec_data, test_labels, use_two_class = True):\n",
    "    train_labels, train_labels = map(np.asarray, [train_labels, train_labels])\n",
    "    if use_two_class:\n",
    "        train_labels, test_labels = map(lambda x: pd.Series(x).map(lambda xx: int(xx <= 2)), [train_labels, test_labels])\n",
    "    clf = None\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "    assert name in [\"SVC\", \"decision_tree\", \"NB\", \"random_forest\", \"MLP\"]\n",
    "    if name == \"SVC\":\n",
    "        #clf = SVC(kernel = \"linear\")\n",
    "        clf = SVC(kernel = \"rbf\")\n",
    "    elif name == \"decision_tree\":\n",
    "        clf = DecisionTreeClassifier()\n",
    "    elif name == \"NB\":\n",
    "        clf = GaussianNB()\n",
    "    elif name == \"random_forest\":\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators = 100, \n",
    "        )\n",
    "    elif name == \"MLP\":\n",
    "        clf = MLPClassifier(hidden_layer_sizes=(300, 300, 300, 150), activation = \"identity\")\n",
    "    clf.fit(train_vec_data, train_labels)\n",
    "    pred = clf.predict(test_vec_data)\n",
    "    acc_score = accuracy_score(test_labels, pred)\n",
    "    balance_acc_score = balanced_accuracy_score(test_labels, pred)\n",
    "    return {\"acc\": acc_score, \"b_acc\": balance_acc_score}\n",
    "\n",
    "traditional_clf_score(\"MLP\", train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
