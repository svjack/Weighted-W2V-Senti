{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_path = \"../data/w2v_douban.model\"\n",
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec.load(w2v_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/svjack/anaconda3/envs/py36/lib/python3.6/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data/DMSC.csv\", index_col = 0)\n",
    "sample_df = data.groupby(\n",
    "    [\"Movie_Name_CN\", \"Star\"]\n",
    ").apply(\n",
    "    lambda x: x.sample(n = int(2125056/(28*200)), replace = True, random_state = 0)\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "comments = sample_df.values[:, 7]\n",
    "star = sample_df.values[:, 6]\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    comments, star, test_size = 0.2, random_state = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read num 100000 set size 71870\n",
      "read num 200000 set size 106359\n",
      "read num 300000 set size 129472\n",
      "read num 400000 set size 146686\n",
      "read num 500000 set size 163406\n",
      "read num 600000 set size 181303\n",
      "read num 700000 set size 197381\n",
      "read num 800000 set size 210911\n",
      "read num 900000 set size 226422\n",
      "read num 1000000 set size 239623\n",
      "read num 1100000 set size 251503\n",
      "read num 1200000 set size 262019\n",
      "read num 1300000 set size 274062\n",
      "read num 1400000 set size 284474\n",
      "read num 1500000 set size 295874\n",
      "read num 1600000 set size 306323\n",
      "read num 1700000 set size 316634\n",
      "read num 1800000 set size 325081\n"
     ]
    }
   ],
   "source": [
    "comment_cut_path = \"../data/douban_comment_cut.txt\"\n",
    "comment_distinct_words = set([])\n",
    "with open(comment_cut_path, \"r\") as f:\n",
    "    idx = 0\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if line:\n",
    "            idx += 1\n",
    "            if idx > 0 and idx % 100000 == 0:\n",
    "                print(\"read num {} set size {}\".format(idx, len(comment_distinct_words)))\n",
    "            for w in line.split(\" \"):\n",
    "                comment_distinct_words.add(w)\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.374 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "wiki_dict_path = '../data/wiki-dict.txt'\n",
    "import jieba\n",
    "jieba.load_userdict(wiki_dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/svjack/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ce347f1efd4c3c99e436cf3aea706f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=42448.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1edf7f92f32747a59b6f8728c3388048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10612.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"nlp-master/stopwords.txt\", \"r\") as f:\n",
    "    stopwords = list(map(lambda line: line.strip(\"\\n\") ,f.readlines()))\n",
    "    \n",
    "def cut(data, labels, stopwords):\n",
    "    result = []\n",
    "    new_labels = []\n",
    "    for index in tqdm_notebook(range(len(data))):\n",
    "        comment = clean_str(data[index])\n",
    "        label = labels[index]\n",
    "        seg_list = jieba.cut(comment, cut_all = False, HMM = True)\n",
    "        seg_list = list(filter(lambda x: x.strip(\"\\n\") if (x not in stopwords and len(x) > 1) else None, seg_list))\n",
    "        if len(seg_list) > 1:\n",
    "            result.append(seg_list)\n",
    "            new_labels.append(label)\n",
    "    return result, new_labels\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import re\n",
    "\n",
    "def clean_str(line):\n",
    "    line.strip('\\n')\n",
    "    line = re.sub(r\"[^\\u4e00-\\u9fff]\", \"\", line)\n",
    "    line = re.sub(\n",
    "        \"[0-9a-zA-Z\\-\\s+\\.\\!\\/_,$%^*\\(\\)\\+(+\\\"\\')]+|[+——！，。？、~@#￥%……&*（）<>\\[\\]:：★◆【】《》;；=?？]+\", \"\", line)\n",
    "    return line.strip()\n",
    "\n",
    "train_cut_result, train_labels = cut(x_train, y_train, stopwords)\n",
    "test_cut_result, test_labels = cut(x_test, y_test, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traditional_clf_score(name, train_vec_data, train_labels, test_vec_data, test_labels, use_two_class = True):\n",
    "    train_labels, train_labels = map(np.asarray, [train_labels, train_labels])\n",
    "    if use_two_class:\n",
    "        train_labels, test_labels = map(lambda x: pd.Series(x).map(lambda xx: int(xx <= 2)), [train_labels, test_labels])\n",
    "    clf = None\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "    assert name in [\"SVC\", \"decision_tree\", \"NB\", \"random_forest\", \"MLP\"]\n",
    "    if name == \"SVC\":\n",
    "        #clf = SVC(kernel = \"linear\")\n",
    "        clf = SVC(kernel = \"rbf\")\n",
    "    elif name == \"decision_tree\":\n",
    "        clf = DecisionTreeClassifier()\n",
    "    elif name == \"NB\":\n",
    "        clf = GaussianNB()\n",
    "    elif name == \"random_forest\":\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators = 100, n_jobs = -1\n",
    "        )\n",
    "    elif name == \"MLP\":\n",
    "        clf = MLPClassifier(hidden_layer_sizes=(300, 300, 300, 150), activation = \"identity\")\n",
    "    clf.fit(train_vec_data, train_labels)\n",
    "    pred = clf.predict(test_vec_data)\n",
    "    acc_score = accuracy_score(test_labels, pred)\n",
    "    balance_acc_score = balanced_accuracy_score(test_labels, pred)\n",
    "    return {\"acc\": acc_score, \"b_acc\": balance_acc_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_df = pd.read_csv(\"../data/douban_word_senti_dict.csv\", index_col=0)\n",
    "req_score_dict = dict((r[\"index\"], r[\"score_mean\"]) for _, r in req_df[[\"score_mean\"]].reset_index().iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original w2v score\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.7400598072876288, 'b_acc': 0.6984274349373714}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#min_value = np.stack(list(map(lambda x: np.concatenate(list(map(lambda y: w2v_model.wv[y].reshape([1, -1]) if y in w2v_model.wv else np.zeros((1,300)), x) ), axis = 0).min(axis = 0),train_cut_result + test_cut_result))).min()\n",
    "#abs_min_value = np.abs(min_value)\n",
    "abs_min_value = 0\n",
    "\n",
    "empty_array = np.stack(list(map(lambda x: np.concatenate(list(map(lambda y: w2v_model.wv[y].reshape([1, -1]) if y in w2v_model.wv else np.zeros((1,300)), x) ), axis = 0).mean(axis = 0),train_cut_result + test_cut_result))).mean(axis = 0) + abs_min_value\n",
    "\n",
    "#assert np.all(empty_array > 0)\n",
    "\n",
    "empty_array = empty_array.reshape((1, 300))\n",
    "#empty_array = np.zeros((1, 300))\n",
    "empty_weight = np.percentile(list(req_score_dict.values()), 50)\n",
    "#empty_weight = 1.0\n",
    "\n",
    "#req_score_dict = dict()\n",
    "\n",
    "train_mean_list = list(map(lambda x: np.concatenate(list(map(lambda y: (w2v_model.wv[y].reshape([1, -1]) + abs_min_value) * \\\n",
    "                                                             1.0 if y in w2v_model.wv else empty_array, x) ), axis = 0).mean(axis = 0),train_cut_result))\n",
    "\n",
    "test_mean_list = list(map(lambda x: np.concatenate(list(map(lambda y: (w2v_model.wv[y].reshape([1, -1]) + abs_min_value) * \\\n",
    "                                                            1.0 if y in w2v_model.wv else empty_array, x) ), axis = 0).mean(axis = 0),test_cut_result))\n",
    "\n",
    "train_X, test_X = np.stack(train_mean_list), np.stack(test_mean_list)\n",
    "\n",
    "train_y, test_y = map(np.asarray, [train_labels, test_labels])\n",
    "print(\"original w2v score\")\n",
    "traditional_clf_score(\"random_forest\", train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v weighted by sentiment dict score\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.7781592645918707, 'b_acc': 0.7567977490118261}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_value = np.stack(list(map(lambda x: np.concatenate(list(map(lambda y: w2v_model.wv[y].reshape([1, -1]) if y in w2v_model.wv else np.zeros((1,300)), x) ), axis = 0).min(axis = 0),train_cut_result + test_cut_result))).min()\n",
    "abs_min_value = np.abs(min_value)\n",
    "#abs_min_value = 0\n",
    "\n",
    "empty_array = np.stack(list(map(lambda x: np.concatenate(list(map(lambda y: w2v_model.wv[y].reshape([1, -1]) if y in w2v_model.wv else np.zeros((1,300)), x) ), axis = 0).mean(axis = 0),train_cut_result + test_cut_result))).mean(axis = 0) + abs_min_value\n",
    "\n",
    "assert np.all(empty_array > 0)\n",
    "\n",
    "empty_array = empty_array.reshape((1, 300))\n",
    "#empty_array = np.zeros((1, 300))\n",
    "empty_weight = np.percentile(list(req_score_dict.values()), 50)\n",
    "#empty_weight = 1.0\n",
    "\n",
    "#req_score_dict = dict()\n",
    "\n",
    "train_mean_list = list(map(lambda x: np.concatenate(list(map(lambda y: (w2v_model.wv[y].reshape([1, -1]) + abs_min_value) * \\\n",
    "                                                             req_score_dict.get(y, empty_weight) if y in w2v_model.wv else empty_array, x) ), axis = 0).mean(axis = 0),train_cut_result))\n",
    "\n",
    "test_mean_list = list(map(lambda x: np.concatenate(list(map(lambda y: (w2v_model.wv[y].reshape([1, -1]) + abs_min_value) * \\\n",
    "                                                            req_score_dict.get(y, empty_weight) if y in w2v_model.wv else empty_array, x) ), axis = 0).mean(axis = 0),test_cut_result))\n",
    "\n",
    "train_X, test_X = np.stack(train_mean_list), np.stack(test_mean_list)\n",
    "\n",
    "train_y, test_y = map(np.asarray, [train_labels, test_labels])\n",
    "print(\"w2v weighted by sentiment dict score\")\n",
    "traditional_clf_score(\"random_forest\", train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v weighted by sentiment dict score add senti agg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.781924908627755, 'b_acc': 0.7595198256500588}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_value = np.stack(list(map(lambda x: np.concatenate(list(map(lambda y: w2v_model.wv[y].reshape([1, -1]) if y in w2v_model.wv else np.zeros((1,300)), x) ), axis = 0).min(axis = 0),train_cut_result + test_cut_result))).min()\n",
    "\n",
    "abs_min_value = np.abs(min_value)\n",
    "\n",
    "empty_array = np.stack(list(map(lambda x: np.concatenate(list(map(lambda y: w2v_model.wv[y].reshape([1, -1]) if y in w2v_model.wv else np.zeros((1,300)), x) ), axis = 0).mean(axis = 0),train_cut_result + test_cut_result))).mean(axis = 0) + abs_min_value\n",
    "\n",
    "assert np.all(empty_array > 0)\n",
    "\n",
    "empty_array = empty_array.reshape((1, 300))\n",
    "#empty_array = np.zeros((1, 300))\n",
    "empty_weight = np.percentile(list(req_score_dict.values()), 50)\n",
    "#empty_weight = 1.0\n",
    "\n",
    "train_mean_list = list(map(lambda x: np.concatenate(list(map(lambda y: (w2v_model.wv[y].reshape([1, -1]) + abs_min_value) * req_score_dict.get(y, empty_weight) if y in w2v_model.wv else empty_array, x) ), axis = 0).mean(axis = 0),train_cut_result))\n",
    "\n",
    "test_mean_list = list(map(lambda x: np.concatenate(list(map(lambda y: (w2v_model.wv[y].reshape([1, -1]) + abs_min_value) * req_score_dict.get(y, empty_weight) if y in w2v_model.wv else empty_array, x) ), axis = 0).mean(axis = 0),test_cut_result))\n",
    "\n",
    "train_X, test_X = np.stack(train_mean_list), np.stack(test_mean_list)\n",
    "\n",
    "train_mean_list = list(map(lambda x: np.concatenate(list(map(lambda y: w2v_model.wv[y].reshape([1, -1]) if y in w2v_model.wv else np.zeros((1,300)), x) ), axis = 0).mean(axis = 0),train_cut_result))\n",
    "test_mean_list = list(map(lambda x: np.concatenate(list(map(lambda y: w2v_model.wv[y].reshape([1, -1]) if y in w2v_model.wv else np.zeros((1,300)), x) ), axis = 0).mean(axis = 0),test_cut_result))\n",
    "\n",
    "train_X_append, test_X_append = np.stack(train_mean_list), np.stack(test_mean_list)\n",
    "\n",
    "train_X = np.concatenate([train_X, train_X_append], axis = 1)\n",
    "\n",
    "test_X = np.concatenate([test_X, test_X_append], axis = 1)\n",
    "\n",
    "train_score_sum = list(map(lambda inner_list: sum(map(lambda x: req_score_dict[x] if x in req_score_dict else empty_weight,inner_list)) ,train_cut_result))\n",
    "\n",
    "test_score_sum = list(map(lambda inner_list: sum(map(lambda x: req_score_dict[x] if x in req_score_dict else empty_weight,inner_list)) ,test_cut_result))\n",
    "\n",
    "train_score_max = list(map(lambda inner_list: max(map(lambda x: req_score_dict[x] if x in req_score_dict else empty_weight,inner_list)) ,train_cut_result))\n",
    "\n",
    "test_score_max = list(map(lambda inner_list: max(map(lambda x: req_score_dict[x] if x in req_score_dict else empty_weight,inner_list)) ,test_cut_result))\n",
    "\n",
    "train_score_min = list(map(lambda inner_list: min(map(lambda x: req_score_dict[x] if x in req_score_dict else empty_weight,inner_list)) ,train_cut_result))\n",
    "\n",
    "test_score_min = list(map(lambda inner_list: min(map(lambda x: req_score_dict[x] if x in req_score_dict else empty_weight,inner_list)) ,test_cut_result))\n",
    "\n",
    "train_score_mean = list(map(lambda inner_list: np.mean(list(map(lambda x: req_score_dict[x] if x in req_score_dict else empty_weight,inner_list))) ,train_cut_result))\n",
    "\n",
    "test_score_mean = list(map(lambda inner_list: np.mean(list(map(lambda x: req_score_dict[x] if x in req_score_dict else empty_weight,inner_list))) ,test_cut_result))\n",
    "\n",
    "train_X = np.concatenate([train_X, np.asarray(train_score_sum).reshape([-1, 1])], axis = 1)\n",
    "\n",
    "test_X = np.concatenate([test_X, np.asarray(test_score_sum).reshape([-1, 1])], axis = 1)\n",
    "\n",
    "train_X = np.concatenate([train_X, np.asarray(train_score_max).reshape([-1, 1])], axis = 1)\n",
    "\n",
    "test_X = np.concatenate([test_X, np.asarray(test_score_max).reshape([-1, 1])], axis = 1)\n",
    "\n",
    "train_X = np.concatenate([train_X, np.asarray(train_score_min).reshape([-1, 1])], axis = 1)\n",
    "\n",
    "test_X = np.concatenate([test_X, np.asarray(test_score_min).reshape([-1, 1])], axis = 1)\n",
    "\n",
    "train_X = np.concatenate([train_X, np.asarray(train_score_mean).reshape([-1, 1])], axis = 1)\n",
    "\n",
    "test_X = np.concatenate([test_X, np.asarray(test_score_mean).reshape([-1, 1])], axis = 1)\n",
    "\n",
    "train_y, test_y = map(np.asarray, [train_labels, test_labels])\n",
    "print(\"w2v weighted by sentiment dict score add senti agg\")\n",
    "traditional_clf_score(\"random_forest\", train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
