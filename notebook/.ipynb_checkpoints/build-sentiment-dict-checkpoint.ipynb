{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/svjack/anaconda3/envs/py36/lib/python3.6/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data/DMSC.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-08-26 15:42:20--  https://codeload.github.com/weiyunchen/nlp/zip/master\n",
      "Resolving codeload.github.com (codeload.github.com)... 13.250.162.133\n",
      "Connecting to codeload.github.com (codeload.github.com)|13.250.162.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/zip]\n",
      "Saving to: ‘master’\n",
      "\n",
      "master                  [   <=>              ]  75.41K   155KB/s    in 0.5s    \n",
      "\n",
      "2020-08-26 15:42:21 (155 KB/s) - ‘master’ saved [77224]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -nc \"https://codeload.github.com/weiyunchen/nlp/zip/master\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  master\r\n",
      "78a55b3ff658cd83cd1ae4abbe5fb475507097c3\r\n",
      "   creating: nlp-master/\r\n",
      "  inflating: nlp-master/faker.png    \r\n",
      "  inflating: nlp-master/stopwords.txt  \r\n"
     ]
    }
   ],
   "source": [
    "!unzip master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nlp-master/stopwords.txt\", \"r\") as f:\n",
    "    stopwords = list(map(lambda line: line.strip(\"\\n\") ,f.readlines()))\n",
    "    \n",
    "def cut(data, labels, stopwords):\n",
    "    result = []\n",
    "    new_labels = []\n",
    "    for index in tqdm_notebook(range(len(data))):\n",
    "        comment = clean_str(data[index])\n",
    "        label = labels[index]\n",
    "        seg_list = jieba.cut(comment, cut_all = False, HMM = True)\n",
    "        seg_list = list(filter(lambda x: x.strip(\"\\n\") if (x not in stopwords and len(x) > 1) else None, seg_list))\n",
    "        if len(seg_list) > 1:\n",
    "            result.append(seg_list)\n",
    "            new_labels.append(label)\n",
    "    return result, new_labels\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import re\n",
    "\n",
    "def clean_str(line):\n",
    "    line.strip('\\n')\n",
    "    line = re.sub(r\"[^\\u4e00-\\u9fff]\", \"\", line)\n",
    "    line = re.sub(\n",
    "        \"[0-9a-zA-Z\\-\\s+\\.\\!\\/_,$%^*\\(\\)\\+(+\\\"\\')]+|[+——！，。？、~@#￥%……&*（）<>\\[\\]:：★◆【】《》;；=?？]+\", \"\", line)\n",
    "    return line.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63711a156423460ebc9404c7d2958d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.838 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import jieba\n",
    "star_sum_dict, star_cnt_dict = defaultdict(float), defaultdict(float)\n",
    "for i ,(index ,row) in tqdm_notebook(enumerate(data[[\"Comment\", \"Star\"]].iterrows())):\n",
    "    comment, label = clean_str(row[\"Comment\"]), row[\"Star\"]\n",
    "    score = float(-1 if label <= 2 else 1)\n",
    "    seg_list = jieba.cut(comment, cut_all = False, HMM = True)\n",
    "    seg_list = list(filter(lambda x: x.strip(\"\\n\") if (x not in stopwords and len(x) > 1) else None, seg_list))\n",
    "    for seg in seg_list:\n",
    "        star_sum_dict[seg] += score\n",
    "        star_cnt_dict[seg] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_star_dict(star_sum_dict, star_cnt_dict):\n",
    "    req = dict()\n",
    "    for word, star_sum in star_sum_dict.items():\n",
    "        req[word] = star_sum / star_cnt_dict[word]\n",
    "    return req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict = merge_star_dict(star_sum_dict, star_cnt_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "sentiment_dict_save_path = \"../data/sentiment_dict.pkl\"\n",
    "if os.path.exists(sentiment_dict_save_path):\n",
    "    os.remove(sentiment_dict_save_path)\n",
    "with open(sentiment_dict_save_path, \"wb\") as f:\n",
    "    pkl.dump({\n",
    "        \"star_sum_dict\": star_sum_dict,\n",
    "        \"star_cnt_dict\": star_cnt_dict, \n",
    "        \"score_dict\": score_dict\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_dict_to_series(d):\n",
    "    return pd.Series(list(d.values()),index=d.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_sum_s = trans_dict_to_series(star_sum_dict)\n",
    "star_cnt_s = trans_dict_to_series(star_cnt_dict)\n",
    "star_df = pd.concat([star_cnt_s, star_sum_s], axis = 1)\n",
    "star_df.columns = [\"cnt\", \"score_sum\"]\n",
    "star_df[\"score_mean\"] = star_df[\"score_sum\"] / star_df[\"cnt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_star_df(star_df):\n",
    "    req = star_df[star_df[\"score_mean\"] > -1.0]\n",
    "    req = req[req[\"score_mean\"] < 1.0]\n",
    "    req = req[req[\"score_mean\"] != 0.0]\n",
    "    return req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_df = filter_star_df(star_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.distplot(req_df[\"score_mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df_path = \"douban_word_senti_dict.csv\"\n",
    "if os.path.exists(score_df_path):\n",
    "    os.remove(score_df_path)\n",
    "req_df.to_csv(score_df_path, index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
