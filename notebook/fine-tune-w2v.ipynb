{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-08-26 15:57:08--  http://wikipedia2vec.s3.amazonaws.com/models/zh/2018-04-20/zhwiki_20180420_300d.txt.bz2\n",
      "Resolving wikipedia2vec.s3.amazonaws.com (wikipedia2vec.s3.amazonaws.com)... 52.219.16.45\n",
      "Connecting to wikipedia2vec.s3.amazonaws.com (wikipedia2vec.s3.amazonaws.com)|52.219.16.45|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 868159128 (828M) [application/x-bzip2]\n",
      "Saving to: ‘zhwiki_20180420_300d.txt.bz2’\n",
      "\n",
      "zhwiki_20180420_300 100%[===================>] 827.94M  1.89MB/s    in 5m 35s  \n",
      "\n",
      "2020-08-26 16:02:45 (2.47 MB/s) - ‘zhwiki_20180420_300d.txt.bz2’ saved [868159128/868159128]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://wikipedia2vec.s3.amazonaws.com/models/zh/2018-04-20/zhwiki_20180420_300d.txt.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bzip2 -d zhwiki_20180420_300d.txt.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wikipedia2vec import Wikipedia2Vec\n",
    "model_ext = Wikipedia2Vec.load_text('zhwiki_20180420_300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "wiki_dict_path = '../data/wiki-dict.txt'\n",
    "if os.path.exists(wiki_dict_path):\n",
    "    os.remove(wiki_dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write num 100000\n",
      "write num 200000\n",
      "write num 300000\n",
      "write num 400000\n",
      "write num 500000\n",
      "write num 600000\n",
      "write num 700000\n",
      "write num 800000\n",
      "write num 900000\n",
      "write num 1000000\n",
      "write num 1100000\n",
      "write num 1200000\n",
      "write num 1300000\n",
      "write num 1400000\n",
      "write num 1500000\n"
     ]
    }
   ],
   "source": [
    "with open(wiki_dict_path, 'w') as f:\n",
    "    for idx, w in enumerate(model_ext.dictionary):\n",
    "        if hasattr(w, \"text\"):\n",
    "            #### skip entity\n",
    "            f.write(u\"{}\\n\".format(w.text))\n",
    "        if idx > 0 and idx % 100000 == 0:\n",
    "            print(\"write num {}\".format(idx))\n",
    "            #break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.264 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "jieba.load_userdict(wiki_dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/svjack/anaconda3/envs/py36/lib/python3.6/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data/DMSC.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import re\n",
    "with open(\"nlp-master/stopwords.txt\", \"r\") as f:\n",
    "    stopwords = list(map(lambda line: line.strip(\"\\n\") ,f.readlines()))\n",
    "def cut(data, labels, stopwords):\n",
    "    result = []\n",
    "    new_labels = []\n",
    "    for index in tqdm_notebook(range(len(data))):\n",
    "        comment = clean_str(data[index])\n",
    "        label = labels[index]\n",
    "        seg_list = jieba.cut(comment, cut_all = False, HMM = True)\n",
    "        seg_list = list(filter(lambda x: x.strip(\"\\n\") if (x not in stopwords and len(x) > 1) else None, seg_list))\n",
    "        if len(seg_list) > 1:\n",
    "            result.append(seg_list)\n",
    "            new_labels.append(label)\n",
    "    return result, new_labels\n",
    "def clean_str(line):\n",
    "    line.strip('\\n')\n",
    "    line = re.sub(r\"[^\\u4e00-\\u9fff]\", \"\", line)\n",
    "    line = re.sub(\n",
    "        \"[0-9a-zA-Z\\-\\s+\\.\\!\\/_,$%^*\\(\\)\\+(+\\\"\\')]+|[+——！，。？、~@#￥%……&*（）<>\\[\\]:：★◆【】《》;；=?？]+\", \"\", line)\n",
    "    return line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_iter(data, stopwords):\n",
    "    assert type(data) == type(pd.Series([]))\n",
    "    for index in tqdm_notebook(range(len(data))):\n",
    "        comment = clean_str(data.iloc[index])\n",
    "        seg_list = jieba.cut(comment, cut_all = False, HMM = True)\n",
    "        seg_list = list(filter(lambda x: x.strip(\"\\n\") if (x not in stopwords and len(x) > 1) else None, seg_list))\n",
    "        if len(seg_list) > 1:\n",
    "            yield seg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_cut_iter = cut_iter(data['Comment'], stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/svjack/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  \n",
      "/home/svjack/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e517c5555e470c923ee5ad5145498b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2125056.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "comment_cut_path = \"../data/douban_comment_cut.txt\"\n",
    "with open(comment_cut_path, 'w') as f:\n",
    "    for idx, seg_list in enumerate(comment_cut_iter):\n",
    "        f.write(u\"{}\\n\".format(\" \".join(seg_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_output_dir = \"../data/douban_comment_cut_split\"\n",
    "import shutil\n",
    "if os.path.exists(split_output_dir):\n",
    "    shutil.rmtree(split_output_dir)\n",
    "os.mkdir(split_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fsplit.filesplit import FileSplit\n",
    "fs = FileSplit(file = comment_cut_path, output_dir = split_output_dir, splitsize=10 * (2**20))\n",
    "fs.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_comment_words_construct(comment_cut_path):\n",
    "    comment_distinct_words = set([])\n",
    "    with open(comment_cut_path, \"r\") as f:\n",
    "        idx = 0\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if line:\n",
    "                idx += 1\n",
    "                if idx > 0 and idx % 100000 == 0:\n",
    "                    print(\"read num {} set size {}\".format(idx, len(comment_distinct_words)))\n",
    "                for w in line.split(\" \"):\n",
    "                    comment_distinct_words.add(w)\n",
    "            else:\n",
    "                break\n",
    "    return comment_distinct_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read num 100000 set size 61204\n",
      "read num 100000 set size 71870\n",
      "read num 100000 set size 70314\n",
      "read num 100000 set size 64985\n",
      "read num 100000 set size 71134\n",
      "read num 100000 set size 64918\n",
      "read num 100000 set size 67528\n",
      "read num 100000 set size 66566\n",
      "read num 100000 set size 59286\n",
      "read num 100000 set size 63806\n",
      "read num 100000 set size 59520\n",
      "read num 100000 set size 69606\n",
      "read num 100000 set size 59126\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from glob import glob\n",
    "import os\n",
    "comment_distinct_words = reduce(lambda a, b: a.union(b) ,map(single_comment_words_construct, glob(os.path.join(split_output_dir, \"*\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flush 10000 into\n",
      "flush 10000 into\n",
      "flush 10000 into\n",
      "flush 10000 into\n",
      "flush 10000 into\n",
      "flush 10000 into\n",
      "flush 10000 into\n",
      "flush 10000 into\n",
      "flush 10000 into\n",
      "flush 10000 into\n",
      "flush 10000 into\n",
      "flush 10000 into\n",
      "flush 10000 into\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(130156, 328151)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "KeyedVectors_ext = KeyedVectors(vector_size=model_ext.get_word_vector(u\"中国\").shape[0])\n",
    "flush_size = 10000 \n",
    "words, vectors = [], []\n",
    "def get_wrapper(word_text):\n",
    "    req = None\n",
    "    try:\n",
    "        req = model_ext.get_word_vector(word_text)\n",
    "    except:\n",
    "        pass\n",
    "    return req\n",
    "for word_text in comment_distinct_words:\n",
    "    vec = get_wrapper(word_text)\n",
    "    if vec is not None:\n",
    "        words.append(word_text)\n",
    "        vectors.append(vec)\n",
    "    if len(words) >= flush_size:\n",
    "        print(\"flush {} into\".format(len(words)))\n",
    "        KeyedVectors_ext.add(words, vectors)\n",
    "        words, vectors = [], []\n",
    "KeyedVectors_ext.add(words, vectors)\n",
    "len(KeyedVectors_ext.vocab), len(comment_distinct_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/svjack/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "328151"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec(size = KeyedVectors_ext.wv[u\"中国\"].shape[0], min_count=1)\n",
    "w2v_model.build_vocab([list(comment_distinct_words)], update=False)\n",
    "len(w2v_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/svjack/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "/home/svjack/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for k in KeyedVectors_ext.vocab.keys():\n",
    "    w2v_model.wv.syn0[w2v_model.wv.vocab[k].index] = KeyedVectors_ext.wv[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_one_time(w2v_model, file_path = None):\n",
    "    if file_path is None:\n",
    "        file_path = np.random.choice(glob(os.path.join(split_output_dir, \"*\")))\n",
    "    sentences = read_tiny_file_to_sentences(file_path)\n",
    "    w2v_model.train(sentences, epochs=5, total_examples=len(sentences))\n",
    "    return w2v_model\n",
    "\n",
    "def read_tiny_file_to_sentences(tiny_file):\n",
    "    with open(tiny_file, \"r\") as f:\n",
    "        return list(map(lambda x: x.strip().split(\" \") ,f.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save 3 to ../data/w2v_douban.model\n",
      "save 6 to ../data/w2v_douban.model\n",
      "save 9 to ../data/w2v_douban.model\n",
      "save 12 to ../data/w2v_douban.model\n",
      "save 15 to ../data/w2v_douban.model\n",
      "save 18 to ../data/w2v_douban.model\n",
      "save 21 to ../data/w2v_douban.model\n",
      "save 24 to ../data/w2v_douban.model\n",
      "save 27 to ../data/w2v_douban.model\n",
      "save 30 to ../data/w2v_douban.model\n",
      "save 33 to ../data/w2v_douban.model\n",
      "save 36 to ../data/w2v_douban.model\n",
      "save 39 to ../data/w2v_douban.model\n",
      "save 42 to ../data/w2v_douban.model\n",
      "save 45 to ../data/w2v_douban.model\n",
      "save 48 to ../data/w2v_douban.model\n"
     ]
    }
   ],
   "source": [
    "run_times = int(1e6)\n",
    "save_every = 3\n",
    "w2v_save_path = \"../data/w2v_douban.model\"\n",
    "if os.path.exists(w2v_save_path):\n",
    "    os.remove(w2v_save_path)\n",
    "for run_time in range(run_times):\n",
    "    w2v_model = iter_one_time(w2v_model)\n",
    "    if run_time > 0 and run_time % save_every == 0:\n",
    "        w2v_model.save(w2v_save_path)\n",
    "        print(\"save {} to {}\".format(run_time ,w2v_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
